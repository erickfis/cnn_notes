# Momentum

# RMS Prop

# Adam - adaptative Momentum estimation

# Learning rate decay

When using minibatches for updating our parameters, the updates might make our values going up and down instead of converging to the minimun.

To smooth this behavior, we could use a lower value for alpha at each iteration.
